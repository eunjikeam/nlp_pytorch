{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'w': 1, 'm': 2, 'f': 3, 't': 4, 'y': 5, 'n': 6, 'o': 7, 'i': 8, 'e': 9, 'u': 10, ' ': 11}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"if you want me\"\n",
    "\n",
    "# make dictionary\n",
    "char_set = list(set(sentence))\n",
    "char_dic = {c:i for i, c in enumerate(char_set)}\n",
    "print(char_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict next word\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, n_layers, batch_first = True)\n",
    "        \n",
    "    def forward(self, X, hidden):\n",
    "        outputs, _status = self.rnn(X, hidden)\n",
    "        return outputs, _status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(char_dic) # input 시 들어가는 character 수(vocab size)\n",
    "hidden_size = len(char_dic) # output 시 나오는 character 수(number of class. 지금은 다음 단어 예측하는 거니까 vocab size 와 동일))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_idx = [char_dic[c] for c in sentence]\n",
    "x_data = [sentence_idx[:-1]] # 마지막 글자 빼고 전부 input\n",
    "x_data = [np.eye(len(char_dic))[x] for x in x_data] # one hot encoding\n",
    "y_data = [sentence_idx[1:]] # 첫번째부터 마지막 까지 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform as torch tensor\n",
    "X = torch.FloatTensor(x_data)\n",
    "y = torch.LongTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare RNN\n",
    "rnn = TextRNN(input_size, hidden_size, 1)\n",
    "\n",
    "# loss& optimizer setting\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, loss : 0.9692, pred str : f you want me\n",
      "Epoch 200/1000, loss : 0.9658, pred str : f you want me\n",
      "Epoch 300/1000, loss : 0.9629, pred str : f you want me\n",
      "Epoch 400/1000, loss : 0.9602, pred str : f you want me\n",
      "Epoch 500/1000, loss : 0.9579, pred str : f you want me\n",
      "Epoch 600/1000, loss : 0.9558, pred str : f you want me\n",
      "Epoch 700/1000, loss : 0.9539, pred str : f you want me\n",
      "Epoch 800/1000, loss : 0.9521, pred str : f you want me\n",
      "Epoch 900/1000, loss : 0.9506, pred str : f you want me\n",
      "Epoch 1000/1000, loss : 0.9491, pred str : f you want me\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    hidden = torch.zeros(1, 1, input_size)\n",
    "    outputs, _status = rnn(X, hidden)\n",
    "    loss = criterion(outputs.view(-1, input_size), y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    pred = outputs.data.numpy().argmax(axis = 2)\n",
    "    pred_str = ''.join([char_set[c] for c in np.squeeze(pred)])\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print('Epoch {}/{}, loss : {:.4f}, pred str : {}'.format(epoch+1, epochs, loss, pred_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 13, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9316, -0.8163, -0.9825, -0.9831, -0.8880, -0.9875, -0.8529, -0.8410,\n",
       "         -0.9766,  0.8771, -0.9913, -0.9711]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:,-1] # hidden_state를 거쳐 나오는 모든 output. shape = [batch_size, time_stemps, n_class]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9316, -0.8163, -0.9825, -0.9831, -0.8880, -0.9875, -0.8529,\n",
       "          -0.8410, -0.9766,  0.8771, -0.9913, -0.9711]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_status # 가장 마지막 hidden state를 거쳐 나온 값. output[:,-1]과 같음.shape = [batch_size, 1, n_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
